# Local Chatbot Lab

## Key Concepts (Understand)
- Running LLMs locally offers privacy and control over data.
- Hardware constraints like GPU memory influence model choice and speed.
- Open-source models can be fine-tuned or configured for custom behavior.

## Deep Learning Technique (Absorb)
**Spaced repetition:** Review commands for launching, querying, and shutting down the local model at increasing intervals.

## Practical Application (Apply)
- Execute the notebook to load a small open-source model and chat with it.
- Modify context length or temperature and note changes in responses.

## Reflection (Integrate)
- In what scenarios would a local model be preferable to a hosted API?

## Connections (Expand)
- [[03 LLM Architecture and Training]]
- [[05 OpenAI Chatbot Lab]]
- External: [Hugging Face Model Hub](https://huggingface.co/models)

## Memory Hooks (Remember)
- Think "local = private" for sensitive data scenarios.
- Visualize GPU memory as a backpack; larger models need bigger packs.

## Backlinks
- [[03 LLM Architecture and Training]]
- [[05 OpenAI Chatbot Lab]]
