# Fine-Tuning

## Key Concepts (Understand)
- Fine-tuning adapts a pre-trained model to a narrower task using additional data.
- Overfitting and catastrophic forgetting are risks when training on small datasets.
- Parameter-efficient methods (LoRA, adapters) reduce compute costs.

## Deep Learning Technique (Absorb)
**Spaced repetition:** Revisit the steps of a fine-tuning pipeline periodically—data prep, training, evaluation, deployment.

## Practical Application (Apply)
- Prepare a small dataset and fine-tune an open-source model on it.
- Compare outputs before and after fine-tuning using the same prompts.

## Reflection (Integrate)
- When is fine-tuning more appropriate than prompt engineering or RAG?

## Connections (Expand)
- [[14 MCP Lab]]
- [[16 Data]]
- External: [Hugging Face Fine-Tuning Guide](https://huggingface.co/docs/transformers/training)

## Memory Hooks (Remember)
- Fine-tuning is "teaching the model new tricks" while retaining old skills.
- Data quality is king—garbage in, garbage out.

## Backlinks
- [[14 MCP Lab]]
- [[16 Data]]
