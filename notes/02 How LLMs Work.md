# How LLMs Work

## Key Concepts (Understand)
- LLMs predict the next word based on all previous words.
- Words are converted to numerical embeddings capturing meaning.
- Training is self-supervised: the model learns from the text itself without human labels.
- Tokens are billing units and building blocks of model input.
- Transformer architecture enables parallel processing of inputs and sequential generation of outputs.

## Deep Learning Technique (Absorb)
**Active recall flashcards:** Create questionâ€“answer pairs such as "What is a token?" or "Why do LLMs use embeddings?" and test yourself.

## Practical Application (Apply)
- Tokenize a paragraph and inspect the tokens using a Python tokenizer.
- Simulate next-word prediction by writing a simple n-gram model.

## Reflection (Integrate)
- How does thinking of a model as a probability machine change how I craft prompts?

## Connections (Expand)
- [[01 Intro to AI]]
- [[03 LLM Architecture and Training]]
- External: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

## Memory Hooks (Remember)
- Picture LLMs as "super autocomplete" systems.
- Embeddings are like coordinates locating words in semantic space.
- Tokens are Lego bricks that build sentences.

## Backlinks
- [[01 Intro to AI]]
- [[03 LLM Architecture and Training]]
